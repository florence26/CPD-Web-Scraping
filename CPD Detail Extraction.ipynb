{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"January\"\n",
    "month_abbr = \"Jan\"\n",
    "month_num = \"01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iStructE Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "title_istructe = []\n",
    "body_1_istructe = []\n",
    "body_2_istructe = []\n",
    "date_istructe = []\n",
    "time_istructe = []\n",
    "link_istructe = []\n",
    "\n",
    "#Looping through the first 2 pages of the iStructE events site\n",
    "for i in range(1,3):\n",
    "    istructe = \"https://www.istructe.org/events/search/?from=&lat=&lng=&maxRadius=50&p=\" + str(i) + \"&s=4&searchTerm=&to=\"\n",
    "    driver.get(istructe)\n",
    "    time.sleep(5)  \n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    for j in soup.find_all(class_='card-promo__description'):\n",
    "        date_ = j.find('div', attrs = {'class': 'card-promo__details'}).text\n",
    "        if month in str(date_):\n",
    "            for k in j.find_all('a', href=True):\n",
    "                link = k['href']\n",
    "                istructe_start = \"https://www.istructe.org\"\n",
    "                event = istructe_start + link\n",
    "                URL = str(event)\n",
    "                link_istructe.append(URL)\n",
    "                \n",
    "                # Opening individual event link\n",
    "                r = requests.get(URL) \n",
    "                soup = BeautifulSoup(r.content, 'html5lib')\n",
    "                \n",
    "                # Title\n",
    "                title = soup.find('h5', attrs = {'class': 'card-summary__title'})\n",
    "                title = title.text\n",
    "                title_istructe.append(title)\n",
    "                \n",
    "                #Description\n",
    "                body_1 = soup.find('div', attrs = {'class':'body-copy'})\n",
    "                body_1 = body_1.text.strip()\n",
    "                body_1_istructe.append(body_1)\n",
    "                \n",
    "                #Date\n",
    "                date_time = soup.find('p').text\n",
    "                date = date_time.split(\"2021\",1)[0]\n",
    "                date_istructe.append(date)\n",
    "                \n",
    "                #Time\n",
    "                time_event = date_time.split(\"2021\",1)[1]\n",
    "                time_istructe.append(time_event)\n",
    "                \n",
    "                #Additional Details\n",
    "                body_2 = soup.find_all('div', attrs = {'class': 'body-copy'})[1]\n",
    "                if body_2.p is None:\n",
    "                    body_2_istructe.append('')\n",
    "                else:\n",
    "                    body_2 = body_2.p.text\n",
    "                    body_2 = body_2.replace(\"\\n\\n\",\" \")\n",
    "                    body_2_istructe.append(body_2)\n",
    "                    \n",
    "istructe_df = pd.DataFrame(list(zip(link_istructe, title_istructe, date_istructe, time_istructe, body_1_istructe, body_2_istructe)), \n",
    "               columns =['Link', 'Title', 'Date', 'Time', 'Description', 'Additional Details']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIBSE events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "links_cibse = []\n",
    "title_cibse = []\n",
    "body_cibse = []\n",
    "date_cibse = []\n",
    "time_cibse = []\n",
    "description_cibse = []\n",
    "\n",
    "# Only looping through the first four pages of the events pages (assume that all of the month's events will be within the first four pages)\n",
    "for k in range(1,4):\n",
    "\n",
    "    URL = \"https://www.cibse.org/training-events?page=\" + str(k) + \"&f=events\"\n",
    "    r = requests.get(URL) \n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    for i in soup.find_all('div', attrs = {'class': 'large-6 columns info'}):\n",
    "        \n",
    "        # Obtaining all the text in each 'div' class with 'large-6 columns info'\n",
    "        info = i.text\n",
    "        \n",
    "        # Month defined at the beginning\n",
    "        if month in str(info):\n",
    "            \n",
    "            # Extracting all links\n",
    "            links = i.a\n",
    "            links = links['href']\n",
    "            event_link = \"https://www.cibse.org\" + links\n",
    "            \n",
    "            # Entering the link of the event\n",
    "            r_event = requests.get(event_link)\n",
    "            soup_event = BeautifulSoup(r_event.content, 'html5lib')\n",
    "            \n",
    "            if 'CIBSE - Page not found' in soup_event.find('body').text:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                links_cibse.append(event_link)\n",
    "                \n",
    "                # Extracting date and time\n",
    "                date_time = soup_event.find('div', attrs = {'class': 'info'}).text\n",
    "                date = date_time.split(\"2021\",1)[0]\n",
    "                date_cibse.append(date.strip())\n",
    "                time = date_time.split(\"2021\",1)[1]\n",
    "                time_cibse.append(time.strip().partition(\"\\n\")[0])\n",
    "\n",
    "                # Extracting title\n",
    "                title = soup_event.find('h1').text\n",
    "                title_cibse.append(title)\n",
    "\n",
    "                # Extracting description\n",
    "                description = soup_event.find('div', attrs = {'class': 'large-8 columns padded-sides body-copy pull-2 float-right'})\n",
    "\n",
    "                description_per_event = []\n",
    "                for k in description.find_all('strong'):\n",
    "                    if k.text != '':\n",
    "                        description_per_event.append(k.text)\n",
    "                description_per_event = ' '.join(description_per_event)\n",
    "                description_cibse.append(description_per_event)\n",
    "\n",
    "                further_description_per_event = []\n",
    "                for j in description.find_all('p'):\n",
    "                    if not j.find_parent('div', attrs = {'class': 'spacer content-block red event-book'}):\n",
    "                        if j.text != '':\n",
    "                            further_description_per_event.append(j.text)\n",
    "                further_description_per_event = ' '.join(further_description_per_event)\n",
    "                body_cibse.append(further_description_per_event)\n",
    "\n",
    "cibse_df = pd.DataFrame(list(zip(links_cibse, title_cibse, date_cibse, time_cibse, description_cibse, body_cibse)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description', 'Additional Details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APM events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "links_apm = []\n",
    "title_apm = []\n",
    "body_apm = []\n",
    "date_apm = []\n",
    "time_apm = []\n",
    "description_apm = []\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "    \n",
    "URL = \"https://www.apm.org.uk/event/\"\n",
    "r = requests.get(URL) \n",
    "\n",
    "driver.get(URL)\n",
    "time.sleep(5)  \n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "# Extracting all links in particular month\n",
    "for i in soup.find('div', attrs = {'class': 'table-responsive'}).find_all(text=re.compile(\"%s\"%month_abbr)):\n",
    "    event_link = i.findParent('td').findPrevious('td').find(href = True)\n",
    "    event_link = \"http://www.apm.org.uk\" + event_link['href']\n",
    "    links_apm.append(event_link)\n",
    "    \n",
    "    r_event = requests.get(event_link)\n",
    "    soup_event = BeautifulSoup(r_event.content, 'html5lib')\n",
    "    \n",
    "    #Extract title and description\n",
    "    title_apm.append(soup_event.find('div', attrs = {'class': 'col-md-8'}).find('h2').text)\n",
    "    description = soup_event.find('div', attrs = {'class': 'col-md-8'}).find(['h2', 'p'])\n",
    "    description_apm.append(description.text)\n",
    "    \n",
    "    body = description.findAllNext(['p','ul'], text = True)\n",
    "    body = [i.text for i in body]\n",
    "    body = ' '.join(body)\n",
    "    body_apm.append(body)\n",
    "    \n",
    "    #Extracting date and time\n",
    "    datetime = soup_event.find('div', attrs = {'class': 'col-md-4'}).find('dl')\n",
    "    datetime = [i for i in datetime]\n",
    "    datetime = [i for i in datetime if (i.string != None)]\n",
    "    date = [i for i in datetime if (str(month) in i.string)]\n",
    "    date = date[0].text.replace(' 2021', '')\n",
    "    date_apm.append(date)\n",
    "    \n",
    "    time_event = [inx for inx, val in enumerate(datetime) if ('Time' in val.string)]\n",
    "    time_apm.append(datetime[time_event[0] + 2].text)\n",
    "    \n",
    "apm_df = pd.DataFrame(list(zip(links_apm, title_apm, date_apm, time_apm, description_apm, body_apm)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description', 'Additional Details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIRCA events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "links_ciria = []\n",
    "title_ciria = []\n",
    "body_ciria = []\n",
    "date_ciria = []\n",
    "time_ciria = []\n",
    "description_ciria = []\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "    \n",
    "URL = \"https://www.ciria.org/CIRIA/Events/Events_overview/Events/Events_overview.aspx?hkey=94fbc810-200b-4650-8d0d-863d30607512\"\n",
    "r = requests.get(URL) \n",
    "\n",
    "driver.get(URL)\n",
    "time.sleep(5)  \n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "#Extract links to all events in month\n",
    "for i in soup.find('div', attrs = {'id': 'ctl01_TemplateBody_WebPartManager1_gwpciNewQueryMenuCommon_ciNewQueryMenuCommon_ResultsGrid_GridPanel1'}).find('tbody').find_all(lambda tag:tag.name==\"td\" and str(month_num) in tag.string):\n",
    "    event_link = i.findNextSibling('td').find('a', href = True)\n",
    "    event_link = event_link['href']\n",
    "    links_ciria.append(event_link)\n",
    "    \n",
    "    r_event = requests.get(event_link)\n",
    "    driver.get(event_link)\n",
    "    time.sleep(5)\n",
    "    soup_event = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    #Extract all titles and descriptions\n",
    "    title = soup_event.find('div', attrs = {'class': 'TitleBarTitle'}).text\n",
    "    title_ciria.append(title.strip())\n",
    "\n",
    "    description_ciria.append(soup_event.find('div', attrs = {'id': 'ctl01_TemplateBody_WebPartManager1_gwpciEventDisplay_ciEventDisplay_DescriptionDiv'}).find('span').text)\n",
    "    \n",
    "    #Extract all date and times\n",
    "    datetime = soup_event.find('div', attrs = {'class': 'Description'}).find(lambda tag:tag.name == 'strong' and tag.string == \"When\").text\n",
    "    \n",
    "    next_datetime = soup_event.find('div', attrs = {'class': 'Description'}).find(lambda tag:tag.string == \"When\").findNextSibling('strong').text\n",
    "    \n",
    "    # Separating out the date and time into individual lists\n",
    "    for j in soup_event.find('div', attrs = {'class': 'Description'}).find_all('p'):\n",
    "        if \"When\" in j.text:\n",
    "            date_time = j.text[j.text.find(datetime)+len(datetime):j.text.rfind(next_datetime)]\n",
    "            date_ciria.append([i for i in date_time.split('\\n') if (i != '')][0].replace(' 2021', ''))\n",
    "            time_ciria.append([i for i in date_time.split('\\n') if (i != '')][1])\n",
    "    \n",
    "ciria_df = pd.DataFrame(list(zip(links_ciria, title_ciria, date_ciria, time_cibse, description_ciria)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CICES events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "links_cices = []\n",
    "title_cices = []\n",
    "date_cices = []\n",
    "time_cices = []\n",
    "description_cices = []\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "    \n",
    "URL = \"https://www.cices.org/news/events/\"\n",
    "r = requests.get(URL) \n",
    "\n",
    "driver.get(URL)\n",
    "time.sleep(5)  \n",
    "soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "#Extracting all data\n",
    "for i in soup.find_all('a', attrs = {'class': 'box event'}):\n",
    "    if str(month_num) in i.attrs[\"data-date\"]:\n",
    "        event_link = i['href']\n",
    "        links_cices.append(event_link)\n",
    "        \n",
    "        r_event = requests.get(event_link)\n",
    "        driver.get(event_link)\n",
    "        time.sleep(5)\n",
    "        soup_event = BeautifulSoup(driver.page_source)\n",
    "        \n",
    "        title = soup_event.find('div', attrs = {'class': 'heading-top'}).text\n",
    "        title_cices.append([i for i in title.split('\\n') if (i != '')][0])\n",
    "        \n",
    "        description = soup_event.find('div', attrs = {'class': 'section section-event'}).find_all('p')\n",
    "        \n",
    "        description = [i.text for i in description]\n",
    "        description = ' '.join(description)\n",
    "        description = description.translate(str.maketrans({'\\n': ' ', '\\t': '', '\\xa0': ' '}))\n",
    "        description_cices.append(description)\n",
    "        \n",
    "        datetime = soup_event.find('div', attrs = {'class': 'column'})\n",
    "        \n",
    "        date_cices.append(datetime.find('h2').text)\n",
    "        \n",
    "        time_cices.append(datetime.find('p').text.replace('\\t','').replace('Starts: ','').replace('Ends:', ' -'))\n",
    "        \n",
    "cices_df = pd.DataFrame(list(zip(links_cices, title_cices, date_cices, time_cices, description_cices)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIHT events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "title_ciht = []\n",
    "description_ciht = []\n",
    "date_ciht = []\n",
    "time_ciht = []\n",
    "link_ciht = []\n",
    "next_month = False\n",
    "page_no = 1\n",
    "\n",
    "# If the month shown on the event is not the month interested, do not go to next page\n",
    "# Extract all links\n",
    "while next_month == False:\n",
    "    ciht = \"https://www.ciht.org.uk/search-results/?events=1&search=#/?page=\" + str(page_no) +\"&filters=%7B%22Region%22:%22%22,%22ContentType%22:%22%22,%22Interests%22:%22%22,%22Upcoming%22:%22True%22,%22Current%22:%22True%22,%22Past%22:%22False%22%7D\"\n",
    "    driver.get(ciht)\n",
    "    time.sleep(5)  \n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    for i in soup.find('div', attrs = {'ng-show': 'stateEvents'}).find_all('div', attrs = {'class': 'item ng-scope'}):\n",
    "        if month in i.text:\n",
    "            event_link = \"https://www.ciht.org.uk/\" + i.find('a')['href']\n",
    "            link_ciht.append(event_link)\n",
    "\n",
    "        else:\n",
    "            next_month = True\n",
    "        \n",
    "    page_no += 1\n",
    "\n",
    "# Extract all other information\n",
    "for i in link_ciht:\n",
    "    r_event = requests.get(i)\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    soup_event = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    details = soup_event.find('div', attrs = {'class': 'col-md-9'})\n",
    "    datetime = details.find('strong').text\n",
    "    datetime = datetime.strip().split(',')\n",
    "    date_ciht.append(datetime[0].replace(' 2021', ''))\n",
    "    time_ciht.append(datetime[1].strip())\n",
    "    \n",
    "    title_ciht.append(details.find('h1').text)\n",
    "    \n",
    "    for j in details.find_all('p', recursive = False):\n",
    "        if j.text.strip() != \"\":\n",
    "            description_ciht.append(j.text.strip().replace(\"\\xa0\", \" \"))\n",
    "            \n",
    "ciht_df = pd.DataFrame(list(zip(link_ciht, title_ciht, date_ciht, time_ciht, description_ciht)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IChemE events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(\"C:/Program Files (x86)/Google/chromedriver.exe\", options=chrome_options)\n",
    "\n",
    "title_icheme = []\n",
    "description_icheme = []\n",
    "date_icheme = []\n",
    "time_icheme = []\n",
    "link_icheme = []\n",
    "next_month = False\n",
    "page_no = 1\n",
    "\n",
    "# If the month shown on the event is not the month interested, do not go to next page\n",
    "# Extract all links\n",
    "while next_month == False:\n",
    "    icheme = \"https://www.icheme.org/membership/communities/all-upcoming-events/page-\" + str(page_no) +\"/\"\n",
    "    driver.get(icheme)\n",
    "    time.sleep(5)  \n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    event = soup.find('div', attrs = {'class': 'event-list-panel-wrap event-list-panel-wrap--m-top'})\n",
    "    \n",
    "    for i in event.find_all('a', href = True):\n",
    "        event_link = i['href']\n",
    "    \n",
    "        if (month in i.text) and (\"Online\" in i.text):\n",
    "            link_icheme.append(\"https://www.icheme.org/\" + event_link)\n",
    "\n",
    "        elif not (month in i.text):\n",
    "            next_month = True\n",
    "        \n",
    "    page_no += 1\n",
    "    \n",
    "# Extract all other information\n",
    "for i in link_icheme:\n",
    "    r_event = requests.get(i)\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    soup_event = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "    title_icheme.append(soup_event.find('h1', attrs = {'class': 'section-title'}).text)\n",
    "    datetime = soup_event.find('ul', attrs = {'class':'event-overview'}).text.split('\\n')\n",
    "    date_icheme.append(datetime[datetime.index('Date From') + 1].replace(' 2021', ''))\n",
    "    the_time = datetime[datetime.index('Location') + 1]\n",
    "    time_icheme.append(re.findall('[0-9][0-9]:[0-9][0-9] [A-Z]{3}', the_time)[0])\n",
    "    \n",
    "    detail = soup_event.find('div', attrs = {'class':'umb-body umb-body--m-bottom'}).find('p').text\n",
    "    description_icheme.append(detail)\n",
    "\n",
    "icheme_df = pd.DataFrame(list(zip(link_icheme, title_icheme, date_icheme, time_icheme, description_icheme)), \\\n",
    "                       columns = ['Link', 'Title', 'Date', 'Time', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge and extract to Excel\n",
    "df = [istructe_df, cibse_df, apm_df, ciria_df,cices_df,ciht_df,icheme_df]\n",
    "df = pd.concat(df)\n",
    "\n",
    "df_compiled = pd.DataFrame()\n",
    "df_compiled['Date and Time'] = df[df.columns[2:4]].apply(lambda x: ','.join(x.dropna()), axis = 1)\n",
    "df_compiled['Title'] = df['Title']\n",
    "df_compiled['Description'] = df.iloc[:,[4,0]].apply(lambda x: ','.join(x.dropna()), axis = 1)\n",
    "\n",
    "df_compiled.to_excel('Events.xlsx', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
